{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>\n",
    "\n",
    "# <center> Class 11, December 1, 2021 </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T20:52:32.045270Z",
     "start_time": "2020-04-21T20:52:32.036041Z"
    }
   },
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "Deep Learning using LeNet and AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "[Comment: LeNet and AlexNet](https://d2l.ai/chapter_convolutional-modern/alexnet.html#fig-alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=\"black\">\n",
    "\n",
    "LeNet\n",
    "\n",
    "<img src=\"./images/cnn-image.jpeg\" width=620 height=620 align = \"center\" >\n",
    "\n",
    "<font size=4 color=\"black\">\n",
    "    \n",
    "LeNet: these networks have a large number of weights and biases; overfitting should be attended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T17:39:09.469844Z",
     "start_time": "2020-04-28T17:39:09.465503Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "[Article: LeNet](./literature/LeNet_lecun-1999.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=\"black\">\n",
    "\n",
    "## AlexNet\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/AlexNet-1.png\" width=620 height=620 align = \"center\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "[Article: AlexNet](./literature/alexnet-paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color=\"black\">\n",
    "\n",
    "## A method to reduce overfitting: Data Augmentation\n",
    "\n",
    "<img src=\"./images/1024px-Regularization.svg.png\" width=300 height=300 align = \"left\" > \n",
    " <img src=\"./images/six-augmented-2.png\" width=520 height=520 align = \"center\" > \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color=\"black\">\n",
    "\n",
    "## Data Augmentation\n",
    "    \n",
    "<font size=4 color=\"black\"> \n",
    "    \n",
    "[Article: Data Augmentation overview](./literature/SurvayData-Augm-DL_2019.pdf)    \n",
    "\n",
    "$$ $$   \n",
    "Deep networks are heavily reliant on big data to avoid overfitting:\n",
    "    \n",
    "Transforming an image\n",
    "    \n",
    "    \n",
    "<img src=\"./images/transforming-a-dog.png\" width=520 height=520 align = \"center\" > \n",
    "\n",
    "\n",
    "Transforming a curve\n",
    "    \n",
    "<img src=\"./images/curve-data-augmentation.png\" width=520 height=520 align = \"center\" > \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "[Keras: Image Preprocessing](https://keras.io/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color=\"black\">\n",
    "\n",
    "## Another way of reducing overfitting is using batch normalization\n",
    "    \n",
    "<font size=4 color=\"black\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "$$ $$\n",
    "    \n",
    "[Article: Batch normalization](./literature/Batch-normalization_2015.pdf)\n",
    "    \n",
    "<img src=\"./images/batch-normalization.png\" width=520 height=520 align = \"center\" >     \n",
    "\n",
    "$$ $$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "Batch normalization helps to reduce the overfitting and accelerates the convergence of the network during training        \n",
    "<img src=\"./images/batch-normalization-formula.png\" width=520 height=520 align = \"center\" > \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T17:54:02.565639Z",
     "start_time": "2020-04-27T17:54:02.558930Z"
    }
   },
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "Deep Learning: LeNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='red'>\n",
    "If you use tensorflow-GPU, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:57.432964Z",
     "start_time": "2021-01-28T12:57:56.184448Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"physical_devices-------------\", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "import pkg_resources\n",
    "def version_library(programa):\n",
    "    return pkg_resources.get_distribution(programa).version\n",
    "programas=['numpy', 'tensorflow']\n",
    "\n",
    "import itertools\n",
    "from itertools import product\n",
    "for v in product(iter(programas)):\n",
    "    print(v[0])\n",
    "    try:\n",
    "        print(version_library(v[0]))\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:57.649432Z",
     "start_time": "2021-01-28T12:57:57.434818Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Activation, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import time\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy version\", np.__version__)\n",
    "print(\"TensorFlow version\", tf.__version__)\n",
    "print(\"Keras version\", keras.__version__)\n",
    "from platform import python_version\n",
    "print(\"Python version\", python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Tensorboard with Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Picture1.png\" width=420 height=420 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color=\"black\">\n",
    "\n",
    "## Data of the System to be analyzed: mnist\n",
    "\n",
    "<font size=4 color=\"black\"> \n",
    "    \n",
    "[The MNIST database](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/MnistExamples-1.png\" width=300 height=300 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Generation or extraction of the raw data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:57.918880Z",
     "start_time": "2021-01-28T12:57:57.651164Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:57.925515Z",
     "start_time": "2021-01-28T12:57:57.920704Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"x_train, y_train type\", type(x_train), type(y_train))\n",
    "print(\"x_test, y_test type\", type(x_test), type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:57.938159Z",
     "start_time": "2021-01-28T12:57:57.927376Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"x_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"x_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Analysis of the raw data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.044462Z",
     "start_time": "2021-01-28T12:57:57.940095Z"
    }
   },
   "outputs": [],
   "source": [
    "image_index = 7777 # You may select anything up to 60,000\n",
    "print(y_train[image_index]) # The label is 8\n",
    "plt.imshow(x_train[image_index], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Transformation of the raw data \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.115517Z",
     "start_time": "2021-01-28T12:57:58.046292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "#input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.123596Z",
     "start_time": "2021-01-28T12:57:58.118879Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Definition of the neural network architecture\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.135082Z",
     "start_time": "2021-01-28T12:57:58.126345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "\n",
    "def architecture(batch_normalization, dropout, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(5,5), input_shape=input_shape))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())    #The recomendaton is to perform batch normalization before activation\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(5,5), input_shape=input_shape))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    if dropout:\n",
    "        model.add(Dropout(0.2))\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())  #The recomendaton is to perform batch normalization before activation\n",
    "    model.add(Activation(activation))\n",
    "\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Generating a model of deep neural network \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "\n",
    "Playing with batch normalization and dropout, you will see that batch normalization improves better the network. Remember that batch normalization is applied before the activation. \n",
    "    \n",
    "[Paper: Batch Normalization](./literature/Batch-normalization_2015.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.802823Z",
     "start_time": "2021-01-28T12:57:58.136952Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_normalization=True\n",
    "dropout=False\n",
    "input_shape = (28, 28, 1)\n",
    "activation = 'relu'\n",
    "\n",
    "LeNet_model = architecture(batch_normalization, dropout, input_shape, activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.928363Z",
     "start_time": "2021-01-28T12:57:58.804361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the architecture\n",
    "\n",
    "plot_model(LeNet_model, to_file='LeNet.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.935286Z",
     "start_time": "2021-01-28T12:57:58.929667Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LeNet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>\n",
    "    \n",
    "[Keras: compiling methods](https://keras.io/models/model/#compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Compiling the model \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:57:58.971796Z",
     "start_time": "2021-01-28T12:57:58.936507Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "LeNet_model.compile(optimizer=optimizers.Adam(learning_rate=lr,beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Running the model \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:07.831014Z",
     "start_time": "2021-01-28T12:57:58.973572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "validation_split = 0.16\n",
    "batch_size = 256\n",
    "num_epochs=20\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs/LeNet\", histogram_freq=1)\n",
    "\n",
    "\n",
    "history = LeNet_model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "                          validation_split=validation_split, shuffle=True,\n",
    "                          callbacks=[tensorboard_callback], verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time for training: {:10.4f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet_model.save('LeNet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Plotting the loss function \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:07.980711Z",
     "start_time": "2021-01-28T12:59:07.836622Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Lr = 0.001, loss_train: 0.1894, \\n loss_val: 1.5591, BatchNorm=True \\n Dropout = 0.4')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "#plt.ylim(top=13)\n",
    "#plt.ylim(bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Plotting the accuracy \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:08.115316Z",
     "start_time": "2021-01-28T12:59:07.982982Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Lr = 0.001, Acc_train: 0.9404, \\n Acc_val: 0.6544 BatchNorm=True \\n Dropout = 0.4')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:08.709335Z",
     "start_time": "2021-01-28T12:59:08.116693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predicting the image associated to the each sample in the test set (X_test)\n",
    "predictions = LeNet_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:08.713570Z",
     "start_time": "2021-01-28T12:59:08.710691Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(predictions))\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:08.726329Z",
     "start_time": "2021-01-28T12:59:08.715098Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = 91\n",
    "print(predictions[sample])\n",
    "print(\"\\nPredicted digit:\", np.argmax(predictions[sample]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\"> \n",
    "Displaying the image associated to this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:08.828812Z",
     "start_time": "2021-01-28T12:59:08.727864Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_test[sample], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    With NETRON\n",
    "$$ $$\n",
    "<font size=4, color='black'>\n",
    "    \n",
    "[How to extract kernel values](https://shivang-ahd.medium.com/how-to-extract-kernel-values-in-cnn-using-netron-and-generate-feature-maps-82cdb6020bb0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NETRON](https://netron.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    With Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T17:54:02.565639Z",
     "start_time": "2020-04-27T17:54:02.558930Z"
    }
   },
   "source": [
    "<font size=3 color=\"black\">\n",
    "\n",
    "## Deep Learning: AlexNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:08.833999Z",
     "start_time": "2021-01-28T12:59:08.830645Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten\n",
    "from tensorflow.keras.layers import Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Picture1.png\" width=420 height=420 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color=\"black\">\n",
    "\n",
    "## Data of the System to be analyzed: oxflowers17\n",
    "\n",
    "<font size=4 color=\"black\"> \n",
    "    \n",
    "[The oxflowers17 database](http://www.robots.ox.ac.uk/~vgg/data/flowers/17/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T20:03:02.613393Z",
     "start_time": "2020-03-30T20:03:02.603220Z"
    }
   },
   "source": [
    "\n",
    "    \n",
    "<font size=4 color=\"black\">\n",
    "$$ $$\n",
    " \n",
    "<img src=\"./images/oxflower17.jpg\" width=500 height=500 align = \"center\" >     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "[Flower classification](./literature/Flower_Classification_2006.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Generation or extraction of the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.load('oxflower17.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = samples['X']\n",
    "train_y = samples['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(train_x), train_x.shape)\n",
    "print(type(train_y), train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T23:38:16.808582Z",
     "start_time": "2020-04-27T23:38:16.804825Z"
    }
   },
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Analysis of the raw data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=4 color='black'>   \n",
    "\n",
    " The oxflower17 dataset consists of 1360 colour images (224 pixels high and 224 pixes width) of flowers in 17 classes, with 80 images per class. 95 % of the images will be used for training. Before running the model, it will be indicated the ratio of samples that will be used for validation.\n",
    "\n",
    "\n",
    "The 17 classes are:\n",
    " \n",
    "| index | class name |\n",
    "| --- | --- |\n",
    "| 0 | Daffodil|\n",
    "| 1 | Snowdrop|\n",
    "| 2 | Daisy|    \n",
    "| 3 | ColtsFoot|\t\t\t\t\t\t\t\t\t\t\n",
    "| 4 | Dandelion|\t\t\t\t\t\t\t\t\t\t\n",
    "| 5 | Cowslip|\n",
    "| 6 | Buttercup|   \n",
    "| 7 | Windflower|\t\t\t\t\t\t\t\t\t\t\n",
    "| 8 | Pansy|\t\t\t\t\t\t\t\t\t\t\n",
    "| 9 | LilyValley|\t\t\t\t\t\t\t\t\t\t\n",
    "|10 | Bluebell |\t\t\t\t\t\t\t\t\t\t\n",
    "|11 | Crocus|\n",
    "|12 | Iris|\t\t\t\t\t\t\t\t\t\t\n",
    "|13 | Tigerlily|\t\t\t\t\t\t\t\t\t\t\n",
    "|14 | Tulip|\t\t\t\t\t\t\t\t\t\t\n",
    "|15 | Fritillary|\n",
    "|16 | Sunflower|\t\t\t\t\t\t\t\t\t\t       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:18:56.606700Z",
     "start_time": "2020-03-24T20:18:56.598151Z"
    }
   },
   "source": [
    "<font size=2 color=\"black\">\n",
    "    \n",
    "## Viewing one sample from the data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T15:54:39.858025Z",
     "start_time": "2020-04-22T15:54:39.848614Z"
    }
   },
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "We define a dictionary to associate the class number to a class name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:09.498071Z",
     "start_time": "2021-01-28T12:59:09.495328Z"
    }
   },
   "outputs": [],
   "source": [
    "dic = {0: 'Daffodil', 1: 'Snowdrop', 2: 'Daisy', 3: 'ColtsFoot', 4: 'Dandelion', \\\n",
    "       5: 'Cowslip', 6: 'Buttercup', 7: 'Windflower', 8: 'Pansy', 9:'LilyValley', \\\n",
    "       10: 'Bluebell', 11: 'Crocus', 12: 'Iris', 13: 'Tigerlily', 14:'Tulip', \\\n",
    "       15: 'Fritillary', 16: 'Sunflower'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "Next, we show a sample: its target and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:09.658260Z",
     "start_time": "2021-01-28T12:59:09.499986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the content of a sample\n",
    "\n",
    "sample = 72\n",
    "\n",
    "plt.imshow(train_x[sample]);\n",
    "print('y =',  np.squeeze(train_y[sample]))\n",
    "\n",
    "for i in [i for i,x in enumerate(train_y[sample]) if x == 1]:\n",
    "    print('')\n",
    "\n",
    "print('y =',  i, ';', 'the sample', sample, 'corresponds to a(an)', dic[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T16:52:07.178780Z",
     "start_time": "2020-04-28T16:52:07.170225Z"
    }
   },
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Transformation of the raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:09.663286Z",
     "start_time": "2021-01-28T12:59:09.659700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('the shape is', train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:09.675986Z",
     "start_time": "2021-01-28T12:59:09.664688Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_x[0][0:5][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T16:52:07.178780Z",
     "start_time": "2020-04-28T16:52:07.170225Z"
    }
   },
   "source": [
    "<font size=4 color='black'>\n",
    "$$ $$    \n",
    "The raw data are renormalized. We do not do anything more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:09.686855Z",
     "start_time": "2021-01-28T12:59:09.677598Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:09.699752Z",
     "start_time": "2021-01-28T12:59:09.688235Z"
    }
   },
   "outputs": [],
   "source": [
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:10.277298Z",
     "start_time": "2021-01-28T12:59:09.701313Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Choose your test size to split between training and testing sets:\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x,train_y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:10.281690Z",
     "start_time": "2021-01-28T12:59:10.278584Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Definition of the neural network architecture\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='black'> \n",
    "    \n",
    "Keras has two different modes to define the architecture:\n",
    "\n",
    "<font size=4 color='black'>     \n",
    "\n",
    "1. The sequential model. It is a sequential stack of layers.\n",
    "$$ $$    \n",
    "2. The functional API. It is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.  \n",
    "$$ $$\n",
    "\n",
    "In the present case, we will use the sequential mode for constructing the architecture of the network.\n",
    "    \n",
    "[Keras: Sequential model API](https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"blac\">\n",
    "    \n",
    "[Keras: Convolutional layers](https://keras.io/layers/convolutional/)\n",
    "$$ $$\n",
    "[Keras: Pooling layers](https://keras.io/layers/pooling/))    \n",
    "$$ $$\n",
    "[Keras: Batch Normalization](https://keras.io/layers/normalization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:10.298604Z",
     "start_time": "2021-01-28T12:59:10.283662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "\n",
    "def architecture(batch_normalization, dropout, input_shape, activation):\n",
    "    \n",
    "    # Creating a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 1st Convolutional layer\n",
    "    model.add(Conv2D(filters=96, activation=activation, input_shape=input_shape,\\\n",
    "      kernel_size=(11,11), strides=(4,4), padding='valid', kernel_initializer='he_uniform'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid')) \n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())  \n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, activation=activation, kernel_size=(5,5), \\\n",
    "                     strides=(1,1), padding='valid'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())  \n",
    "    \n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, activation=activation, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "     \n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, activation=activation, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    \n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, activation=activation, kernel_size=(3,3), strides=(1,1), padding='valid'))    \n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())   \n",
    "\n",
    "    # Passing it to a dense layer\n",
    "    model.add(Flatten())\n",
    "    if dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(512, activation=activation, input_shape=(224*224*3,), kernel_initializer = 'he_uniform'))\n",
    "    # Add Dropout to prevent overfitting\n",
    "    if dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())   \n",
    "    \n",
    "    # 2nd Dense Layer\n",
    "    model.add(Dense(512, activation=activation, kernel_initializer = 'he_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    if dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    if batch_normalization: \n",
    "        model.add(BatchNormalization())   \n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(17, activation='softmax'))\n",
    "              \n",
    "    return model\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T18:29:47.071906Z",
     "start_time": "2020-03-24T18:29:47.067719Z"
    }
   },
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Generating a model of deep neural network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:10.692440Z",
     "start_time": "2021-01-28T12:59:10.300034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating the model using the defined architecture\n",
    "\n",
    "batch_normalization=True\n",
    "dropout=True\n",
    "one_image = (224, 224, 3)\n",
    "activation = 'relu'\n",
    "\n",
    "oxflower17_model = architecture(batch_normalization, dropout, one_image, activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:10.877247Z",
     "start_time": "2021-01-28T12:59:10.693775Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(oxflower17_model, to_file='oxflower17_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:10.888226Z",
     "start_time": "2021-01-28T12:59:10.879280Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "oxflower17_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T17:05:29.967758Z",
     "start_time": "2020-04-28T17:05:29.958426Z"
    }
   },
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T12:59:10.950235Z",
     "start_time": "2021-01-28T12:59:10.889342Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compiling the model using Adam as optimizer\n",
    "\n",
    "lr = 0.0001  # Learning rate\n",
    "\n",
    "oxflower17_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], \\\n",
    "optimizer=optimizers.Adam(learning_rate=lr,beta_1=0.9, beta_2=0.999, amsgrad=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>   \n",
    "    \n",
    "Checkpoint callback usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_2/oxflower17_cp.cpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Creating checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                      save_weights_only=True,\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>   \n",
    "    \n",
    "TensorBoard callback usage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating  TensorBoard callback\n",
    "\n",
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs/oxflower17\", histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Running the model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:14.387009Z",
     "start_time": "2021-01-28T12:59:10.952204Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "batch_size=128\n",
    "num_epochs = 50\n",
    "val_split= 0.1\n",
    "\n",
    "\n",
    "history_1 = oxflower17_model.fit(train_x, train_y, batch_size=batch_size, epochs=num_epochs,\n",
    "                               validation_split=val_split, shuffle=True,\n",
    "                               callbacks=[tb_callback, cp_callback], verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time for training: {:10.4f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxflower17_model.save('oxflower17_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = oxflower17_model.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:36:05.683462Z",
     "start_time": "2020-04-22T12:36:05.671011Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "* Note: if you run `fit()` again, the `model` will continue training, starting with the parameters it has already learnt, instead of reinitializing them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Plotting the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:14.540355Z",
     "start_time": "2021-01-28T13:02:14.391380Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['loss'], color='red')\n",
    "plt.plot(history_1.history['val_loss'], color='blue')\n",
    "plt.title('Lr = 0.0001, BatchNorm=True \\n Dropout = 0.5')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.ylim(top=4)    # The instruction is used to limit the upper value of the loss function \n",
    "plt.ylim(bottom=0)  # The instruction is used to limit the lower value of the loss function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Plotting the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:14.681997Z",
     "start_time": "2021-01-28T13:02:14.541722Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['accuracy'], color='red')\n",
    "plt.plot(history_1.history['val_accuracy'], color='blue')\n",
    "plt.title('Lr = 0.0001, BatchNorm=True \\n Dropout = 0.5')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    With NETRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NETRON](https://netron.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    With Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/oxflower17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "Calling back the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest\n",
    "oxflower17_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = oxflower17_model.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color=\"black\">\n",
    "    \n",
    "## Data augmentation\n",
    "\n",
    "<font size=4 color=\"black\">\n",
    "$$ $$\n",
    "shear_range, zoom_range, and horizontal_flip are some of the parameter available in Keras that define the transformation of the images\n",
    "\n",
    "[Keras: Data augmetation](https://keras.io/preprocessing/image/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_x))\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_y))\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "\n",
    "[Comment: Keras ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:15.014459Z",
     "start_time": "2021-01-28T13:02:15.012151Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:15.151280Z",
     "start_time": "2021-01-28T13:02:15.016066Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "train_datagen.fit(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T18:29:47.071906Z",
     "start_time": "2020-03-24T18:29:47.067719Z"
    }
   },
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Generating a model of deep neural network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:14.969343Z",
     "start_time": "2021-01-28T13:02:14.683222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating the model using the defined architecture\n",
    "\n",
    "batch_normalization=True\n",
    "dropout=True\n",
    "one_image = (224, 224, 3)\n",
    "activation = 'relu'\n",
    "\n",
    "oxflower17_augm = architecture(batch_normalization, dropout, one_image, activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T17:05:29.967758Z",
     "start_time": "2020-04-28T17:05:29.958426Z"
    }
   },
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:15.010976Z",
     "start_time": "2021-01-28T13:02:14.970640Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compiling the model using Adam as optimizer\n",
    "\n",
    "lr = 0.0001  # Learning rate\n",
    "\n",
    "oxflower17_augm.compile(loss='categorical_crossentropy', metrics=['accuracy'], \\\n",
    "optimizer=optimizers.Adam(learning_rate=lr,beta_1=0.9, beta_2=0.999, amsgrad=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./augmented_images/\n",
    "!mkdir ./augmented_images/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color='black'>\n",
    "\n",
    "##  Running the model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "\n",
    "[Comment: Keras flow method](https://theailearner.com/2019/07/06/imagedatagenerator-flow-method/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "This process requires long times, depending of the number of steps per epoch, the number of epochs and the number of images that will be generated during the data augmentation (batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:15.151280Z",
     "start_time": "2021-01-28T13:02:15.016066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Real time generation of augmented data for trainig\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size = 32,\n",
    "    shuffle=True,\n",
    "    save_to_dir='augmented_images',\n",
    "    save_prefix='aug',\n",
    "    save_format='png',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real time generation of augmented data for validation\n",
    "\n",
    "val_generator = train_datagen.flow(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size =16,\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:02:15.156385Z",
     "start_time": "2021-01-28T13:02:15.152573Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_steps_augment = 128 \n",
    "\n",
    "print (\"train_x shape: \" + str(train_x.shape[0]))\n",
    "steps = int(train_x.shape[0]/get_steps_augment)\n",
    "print(\"Augmentation steps = {}\".format(steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:10:58.420570Z",
     "start_time": "2021-01-28T13:02:15.158367Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs/oxflower17_augm\", histogram_freq=1)\n",
    "\n",
    "\n",
    "history_2 = oxflower17_augm.fit(train_generator,\n",
    "                              validation_data=val_generator,\n",
    "                              steps_per_epoch=steps,\n",
    "                              epochs=num_epochs,\n",
    "                              shuffle=True,\n",
    "                              callbacks=[tensorboard_callback], verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time for training: {:10.4f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxflower17_augm.save('oxflower17_augment.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:36:05.683462Z",
     "start_time": "2020-04-22T12:36:05.671011Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "* Note: if you run `fit()` again, the `model` will continue training, starting with the parameters it has already learnt, instead of reinitializing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:10:58.567974Z",
     "start_time": "2021-01-28T13:10:58.422068Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['loss'], color='red')\n",
    "plt.plot(history_2.history['val_loss'], color='blue')\n",
    "plt.title('Lr = 0.0001, BatchNorm=True \\n Dropout = 0.4')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.ylim(top=4)    # The instruction is used to limit the upper value of the loss function \n",
    "plt.ylim(bottom=0)  # The instruction is used to limit the lower value of the loss function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:10:58.832223Z",
     "start_time": "2021-01-28T13:10:58.569926Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['accuracy'], color='red')\n",
    "plt.plot(history_2.history['val_accuracy'], color='blue')\n",
    "plt.title('Lr = 0.0001, BatchNorm=True \\n Dropout = 0.4')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:10:59.206728Z",
     "start_time": "2021-01-28T13:10:58.834084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predicting the image associated to the each sample in the test set (X_test)\n",
    "predictions = oxflower17_augm.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:10:59.211983Z",
     "start_time": "2021-01-28T13:10:59.208469Z"
    }
   },
   "outputs": [],
   "source": [
    "print(type(predictions))\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-28T13:10:59.376676Z",
     "start_time": "2021-01-28T13:10:59.213930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predicting the image associated to the sample \n",
    "# np.argmax returns the index of the maximum value\n",
    "sample = 17\n",
    "prediction = np.argmax(predictions[sample])\n",
    "print(\"Prediction number=\", prediction, ', it corresponds to a', dic[prediction])\n",
    "\n",
    "\n",
    "# Plotting the content of a sample\n",
    "\n",
    "plt.imshow(train_x[sample]);\n",
    "print('\\ny =',  np.squeeze(train_y[sample]))\n",
    "\n",
    "for i in [i for i,x in enumerate(train_y[sample]) if x == 1]:\n",
    "    print('')\n",
    "\n",
    "print('y =',  i, ';', 'the sample', sample, 'corresponds to a(an)', dic[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NETRON](https://netron.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    With Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/oxflower17_augm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "The following command will find the tensorbroad process and terminate it:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!kill $(ps -e | grep 'tensorboard' | awk '{print $1}')\n",
    "!kill $(ps -e | grep 'tensorboard' | awk '{print $1}')\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "If you execute the above command, it would be impossible to use tensorboard for the analysis. \n",
    "    \n",
    "The above command avoids the accumulation of information when the Learning Machine is trained again.\n",
    "When you finish a notebook run, execute the command to stop the execution of the app tensorboard.\n",
    "\n",
    "To get a better understanding of the command, run the notebook several times, without executing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 >\n",
    "Keras manages a global state, which it uses to implement the Functional model-building API and to uniquify autogenerated layer names.  \n",
    "If you are creating many models in a loop, this global state will consume an increasing amount of memory over time, and you may want to clear it. Calling clear_session() releases the global state: this helps avoid clutter from old models and layers, especially when memory is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
