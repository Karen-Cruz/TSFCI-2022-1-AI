{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color='blue'>\n",
    "\n",
    "# <center> Class 13-1, December 15, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "Connection Geometries in Neural Networks\n",
    "    \n",
    "<font size=4 color=\"black\">\n",
    "$$ $$ \n",
    "In the architecture of a neural network, it is important to know where the connections originate and terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "Neural Networks with shortcut connections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "\n",
    "In many problems, the contribution to the response of the input variables is often linear, and nonlinear effects often coincide with small deviations from a linear solution. Such a mixed contribution to the response can be handled with a neural network with shorcut connections; i. e. the input variable to a set of layers is also connected to the output of a set. The set of layers models the nonlinear effect associated to the input variable.\n",
    "\n",
    "<img src=\"./images/FF-with-shortcuts.jpg\" width=400 height=400 align = \"left\" >    \n",
    "<img src=\"./images/FF-with-shortcuts-1.png\" width=350 height=300 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "[Reference: Spirals separation](./literature/shortcuts_Learning-two-spirals-apart_1989.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=2 color=\"black\">\n",
    "\n",
    "## ResNet\n",
    "\n",
    "<img src=\"./images/resnet-keras.png\" width=400 height=400 align = \"center\" >\n",
    "\n",
    "<font size=4 color=\"black\">\n",
    "$$ $$\n",
    "Residual Learning: a learning block. F(X) represents the nonlinear response of the input X.\n",
    "$$ $$    \n",
    "\n",
    "[Reference: ResNet](./literature/RCNN_2016.pdf)\n",
    "    \n",
    "[Comment: ResNet in Keras](https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T19:24:37.815111Z",
     "start_time": "2020-05-04T19:24:37.803050Z"
    }
   },
   "source": [
    "\n",
    "<img src=\"./images/resnet-1.png\" width=920 height=620 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T17:39:09.469844Z",
     "start_time": "2020-04-28T17:39:09.465503Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "[Comment: ResNet](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=4 color=\"black\">\n",
    "\n",
    "With the architecture of the present notebook, we can construct ResNet networks with different number of Layers (indicated with the number).\n",
    "$$ $$\n",
    "ResNet-20 \n",
    "\n",
    "ResNet-32\n",
    "    \n",
    "ResNet-44\n",
    "    \n",
    "ResNet-56\n",
    "    \n",
    "ResNet-110\n",
    "    \n",
    "ResNet-164\n",
    "    \n",
    "ResNet-1001\n",
    "    \n",
    "The execution time of the neural network depends on the number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Dynamical System Evolution](./literature/resnet_applications/Dynamical_system_evolution_2019.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Health Monitoring](./literature/resnet_applications/health_monitoring_2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Land Use Detection](./literature/resnet_applications/Land-use-detection_2019.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Time Series Classification](./literature/resnet_applications/Timeseries_classification.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "Extensions of ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T19:10:33.427815Z",
     "start_time": "2020-05-04T19:10:33.423600Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "\n",
    "[Reference: Adjustable shorcut connections](./literature/Improving_ResNet_2018.pdf)\n",
    "    \n",
    "[Reference: ResNeXt](./literature/ResNeXt_2017.pdf)\n",
    "\n",
    "[Reference: DenseNet](./literature/DEnse_CNN2017.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T17:54:02.565639Z",
     "start_time": "2020-04-27T17:54:02.558930Z"
    }
   },
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "Deep Learning: ResNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"physical_devices-------------\", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "import pkg_resources\n",
    "def version_library(programa):\n",
    "    return pkg_resources.get_distribution(programa).version\n",
    "programas=['numpy', 'tensorflow','pydot', 'matplotlib', 'pickle']\n",
    "\n",
    "import itertools\n",
    "from itertools import product\n",
    "for v in product(iter(programas)):\n",
    "    print(v[0])\n",
    "    try:\n",
    "        print(version_library(v[0]))\n",
    "    except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:39.375398Z",
     "start_time": "2020-05-06T20:09:35.447230Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "#from keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy version\", np.__version__)\n",
    "print(\"TensorFlow version\", tf.__version__)\n",
    "print(\"Keras version\", keras.__version__)\n",
    "#print(\"Pydot version\", pydot.__version__)\n",
    "print(\"Matplotlib version\", matplotlib.__version__)\n",
    "#print(\"Pickle version\", pickle.format_version)\n",
    "from platform import python_version\n",
    "print(\"Python version\", python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Tensorboard with Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting possible active application of tensorboard\n",
    "!kill $(ps -e | grep 'tensorboard' | awk '{print $1}')\n",
    "!kill $(ps -e | grep 'tensorboard' | awk '{print $1}')\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_resnet/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Picture1.png\" width=420 height=420 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Object Recognition through images using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cifar-10.png\" width=420 height=420 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "CIFAR-10 database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "\n",
    "Generation or extraction of the raw data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:39.379349Z",
     "start_time": "2020-05-06T20:09:39.376774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.161016Z",
     "start_time": "2020-05-06T20:09:39.382006Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.165983Z",
     "start_time": "2020-05-06T20:09:40.162929Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Analysis of the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>   \n",
    " The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. $$ $$\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each training batch with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "The ten classes are:\n",
    "\n",
    "| index | class name |\n",
    "| --- | --- |\n",
    "| 0 | airplane |\n",
    "| 1 | automobile | \t\t\t\t\t\t\t\t\t\t\n",
    "| 2 | bird |\t\t\t\t\t\t\t\t\t\t\n",
    "| 3 | cat |\t\t\t\t\t\t\t\t\t\t\n",
    "| 4 | deer |\t\t\t\t\t\t\t\t\t\t\n",
    "| 5 | dog |\t\t\t\t\t\t\t\t\t\t\n",
    "| 6 | frog |\t\t\t\t\t\t\t\t\t\t\n",
    "| 7 | horse |\t\t\t\t\t\t\t\t\t\t\n",
    "| 8 | ship |\t\t\t\t\t\t\t\t\t\t\n",
    "| 9 | truck|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T15:54:39.858025Z",
     "start_time": "2020-04-22T15:54:39.848614Z"
    }
   },
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "We define a dictionary to associate the class number to a class name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.177272Z",
     "start_time": "2020-05-06T20:09:40.167193Z"
    }
   },
   "outputs": [],
   "source": [
    "# We define a dictionary to associate the class number to a class name.\n",
    "\n",
    "dic = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', \n",
    "       5: 'dog', 6: 'frog', 7:'horse', 8: 'ship', 9: 'truck'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:18:56.606700Z",
     "start_time": "2020-03-24T20:18:56.598151Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "Viewing one sample from the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.191942Z",
     "start_time": "2020-05-06T20:09:40.178922Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T20:05:48.410513Z",
     "start_time": "2020-04-29T20:05:48.401045Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "Every value of X in the example is between 0 and 255, they are not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.205520Z",
     "start_time": "2020-05-06T20:09:40.193215Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "Next, we show a sample: its target and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.357945Z",
     "start_time": "2020-05-06T20:09:40.207947Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = 9\n",
    "\n",
    "plt.imshow(x_train[sample]);\n",
    "\n",
    "print('y =',  np.squeeze(y_train[sample]), ';', 'the sample', sample,\\\n",
    "      'corresponds to a', dic[int(np.squeeze(y_train[sample]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T16:52:07.178780Z",
     "start_time": "2020-04-28T16:52:07.170225Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Transformation of the raw data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "$$ $$\n",
    "To normalize the X values of an image, we divide each pixel value by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.820665Z",
     "start_time": "2020-05-06T20:09:40.360149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:40.824480Z",
     "start_time": "2020-05-06T20:09:40.822073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.064407Z",
     "start_time": "2020-05-06T20:09:40.826436Z"
    }
   },
   "outputs": [],
   "source": [
    "# If subtract pixel mean is enabled\n",
    "\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.070300Z",
     "start_time": "2020-05-06T20:09:41.065606Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.082638Z",
     "start_time": "2020-05-06T20:09:41.071828Z"
    }
   },
   "outputs": [],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.094840Z",
     "start_time": "2020-05-06T20:09:41.083891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.106327Z",
     "start_time": "2020-05-06T20:09:41.096043Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.120654Z",
     "start_time": "2020-05-06T20:09:41.107547Z"
    }
   },
   "outputs": [],
   "source": [
    "print('y_train shape =', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Definition of the neural network architecture\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Learning Rate Schedule\n",
    "\n",
    "Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs. Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.135609Z",
     "start_time": "2020-05-06T20:09:41.121998Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    \n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "2D Convolution-Batch Normalization-Activation stack builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.147434Z",
     "start_time": "2020-05-06T20:09:41.137119Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    '''\n",
    "        # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    '''\n",
    "    \n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "\n",
    "\n",
    "    \n",
    "ResNet Version 1 Model, accoding to the following paper: [Model 1](./literature/RCNN_2016.pdf) \n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number of filters and the\n",
    "    same filter map sizes.\n",
    "    \n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    \n",
    "    The Number of parameters is approx the same as Table 6 of paper cited above.\n",
    "    \n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.165513Z",
     "start_time": "2020-05-06T20:09:41.148695Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \n",
    "    '''\n",
    "    # Arguments\n",
    "    input_shape (tensor): shape of input image tensor\n",
    "    depth (int): number of core convolutional layers\n",
    "    num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "    model (Model): Keras model instance\n",
    "    '''\n",
    "    \n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "    print('The number of stages = ', num_res_blocks,'\\n' )\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "   \n",
    "ResNet Version 2 Model. According to the following paper: [Model 2](./literature/Model2_2016.pdf)\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number of filters and the\n",
    "    same filter map sizes.\n",
    "    \n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.178958Z",
     "start_time": "2020-05-06T20:09:41.166902Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \n",
    "    '''\n",
    "    # Arguments\n",
    "    input_shape (tensor): shape of input image tensor\n",
    "    depth (int): number of core convolutional layers\n",
    "    num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "    model (Model): Keras model instance\n",
    "    '''\n",
    "    \n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "    print('The number of stages = ', num_res_blocks, '\\n')\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T18:29:47.071906Z",
     "start_time": "2020-03-24T18:29:47.067719Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Generating a model of deep neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "\n",
    "Choosing between Model1 and Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:41.196729Z",
     "start_time": "2020-05-06T20:09:41.180318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summary of the results described in the papers of Model1 and Model2\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Model version\n",
    "\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# This is valid only for ResNet20. The respective values should be changed for another version of ResNet \n",
    "\n",
    "if version == 1:\n",
    "    n = 3    # For Model ResNet20 version 1\n",
    "elif version == 2:\n",
    "    n = 2    # For Model ResNet20 version 2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:45.906974Z",
     "start_time": "2020-05-06T20:09:41.198333Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "\n",
    "if version == 2:\n",
    "    resnet_model = resnet_v2(input_shape=input_shape, depth=depth, num_classes = num_classes)\n",
    "else:\n",
    "    resnet_model = resnet_v1(input_shape=input_shape, depth=depth, num_classes = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:45.913149Z",
     "start_time": "2020-05-06T20:09:45.910215Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"The number of layers in depth is \", depth)\n",
    "print(\"It excludes the pooling and dense (full connected) layers \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:46.934510Z",
     "start_time": "2020-05-06T20:09:46.929949Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model name, depth and version\n",
    "\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model chacking saving directory.\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:46.946989Z",
     "start_time": "2020-05-06T20:09:46.937186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare model name\n",
    "model_name_0 = 'cifar10_%s_model' % model_type\n",
    "print(model_name_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:46.809027Z",
     "start_time": "2020-05-06T20:09:45.914755Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_model(resnet_model, to_file=model_name_0+'.png', show_shapes=True, rankdir='TB', expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:46.829616Z",
     "start_time": "2020-05-06T20:09:46.810795Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T17:05:29.967758Z",
     "start_time": "2020-04-28T17:05:29.958426Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:46.928115Z",
     "start_time": "2020-05-06T20:09:46.831270Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compiling the model using Adam as optimizer\n",
    "\n",
    "resnet_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Callbacks $$ $$\n",
    "    \n",
    "<font size=4 color='black'> \n",
    "    \n",
    "[Keras.callbacks](https://www.tensorflow.org/guide/keras/custom_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include tf.keras.callbacks.TensorBoard to visualize training progress and results with TensorBoard, or tf.keras.callbacks.ModelCheckpoint to periodically save your model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "All callbacks subclass the keras.callbacks.Callback class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\n",
    "\n",
    "You can pass a list of callbacks (as the keyword argument callbacks) to the following model methods:\n",
    "\n",
    "    keras.Model.fit()\n",
    "    keras.Model.evaluate()\n",
    "    keras.Model.predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Learning rate scheduling using callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "In the next cell, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:46.958058Z",
     "start_time": "2020-05-06T20:09:46.948512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs_resnet/model_name_0\", histogram_freq=1)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler, tensorboard_callback]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Running the model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:09:46.970118Z",
     "start_time": "2020-05-06T20:09:46.959832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "data_augmentation = False\n",
    "batch_size = 32  # original paper trained all networks with batch_size=128\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:12:12.888486Z",
     "start_time": "2020-05-06T20:09:46.972292Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run training, with or without data augmentation.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history=resnet_model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    \n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    history=resnet_model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs, verbose=1, workers=4,\n",
    "                        callbacks=callbacks + tensorboard_callback)\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Time for training: {:10.4f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.save(model_name_0+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:12:17.750580Z",
     "start_time": "2020-05-06T20:12:12.892626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = resnet_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:36:05.683462Z",
     "start_time": "2020-04-22T12:36:05.671011Z"
    }
   },
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "* Note: if you run `fit()` again, the `model` will continue training, starting with the parameters it has already learnt, instead of reinitializing them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plotting the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:12:17.909430Z",
     "start_time": "2020-05-06T20:12:17.753296Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Lr = 0.001, loss_train: 0.1665, \\n loss_val: 0.4389, BatchNorm=True \\n Data_augmentation=True')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "#plt.ylim(top=13)    # The instruction is used to limit the upper value of the loss function \n",
    "#plt.ylim(bottom=0)  # The instruction is used to limit the lower value of the loss function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plotting the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:12:18.070460Z",
     "start_time": "2020-05-06T20:12:17.910996Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Lr = 0.001, Acc_train: 0.9860, \\n Acc_val: 0.9174 BatchNorm=True \\n Data_augmentation=True')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    With NETRON\n",
    "$$ $$\n",
    "<font size=4, color='black'>\n",
    "    \n",
    "[How to extract kernel values](https://shivang-ahd.medium.com/how-to-extract-kernel-values-in-cnn-using-netron-and-generate-feature-maps-82cdb6020bb0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NETRON](https://netron.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5, color=blue>\n",
    "    With Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs_resnet/model_name_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=\"black\">\n",
    "    \n",
    "[Summary of the runs for 200 epochs](Clase-13.odt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
