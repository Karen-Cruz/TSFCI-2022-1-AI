{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>\n",
    "\n",
    "# <center> Clase 13-2, diciembre 15, 2021 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=\"blue\">\n",
    "\n",
    "Transferencia de estilo entre imágenes utilizando una red neuronal VGG-19\n",
    "$ $\n",
    "<font size=4 color='black'> \n",
    "\n",
    "[Referencia: VGG-19](./literature/Very-deep-CNN_2015.pdf) $$ $$\n",
    "    \n",
    "[Referencia: A Neural Algorithm of Artistic Style](./literature/A-Neural-Algorithm-of-Artistic-Style.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"./images/Vgg-19.png\" width=620 height=620 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "[Tutorial de Keras sobre transferencia de estilo](https://keras.io/examples/generative/neural_style_transfer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Aplicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Detection of Wearing of Masks](./literature/vgg19_applications/detection-wearing-masks_2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Fundus_Image classification](./literature/vgg19_applications/Fundus_image-classification_2018.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Fault Diagnosis](./literature/vgg19_applications/fault_diagnosis_2019.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'> \n",
    "    \n",
    "[Referencia: Smart-Phone Based Waste Classification](./literature/vgg19_applications/waste-classification_2020.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Aprender-no-supervisado.png\" width=420 height=420 align = \"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Transferencia de estilo entre imágenes via neuronas (NST, Neural Style Transfer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Antecedentes:\n",
    "[Referencia: Image quilting 2001](./literature/Image-quilting_2001.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Se tiene una imágen original y una imagen con el estilo que se va a transferir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "[Referencia: A Neural Algorithm of Artistic Style](./literature/A-Neural-Algorithm-of-Artistic-Style.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Este es un ejemplo de aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "#This is an alternative way to get images from URLs\n",
    "\n",
    "#Imagen base\n",
    "base_image_path = keras.utils.get_file(\"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\")\n",
    "print(type(base_image_path))\n",
    "#Imagen de estilo\n",
    "style_reference_image_path = keras.utils.get_file(\"starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Imagen base\n",
    "base_image_path = \"./images/Green_Sea_Turtle_grazing_seagrass.jpg\"\n",
    "#Imagen de estilo\n",
    "style_reference_image_path = \"./images/oxtotitlan.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagen base\n",
    "base_image_path = \"./images/Green_Sea_Turtle_grazing_seagrass.jpg\"\n",
    "#Imagen de estilo\n",
    "style_reference_image_path = \"./images/maya.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "print(\"Tamaño de la imagen de base: Ancho: \", width, \" Alto: \",height)\n",
    "\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "width, height = keras.preprocessing.image.load_img(style_reference_image_path).size\n",
    "print(\"Tamaño de la imagen que transfiere el estilo: Ancho: \", width, \" Alto: \",height)\n",
    "\n",
    "print(\"Tamaño de la imagen procesada: Ancho:\",img_ncols,\" Alto: \",img_nrows )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "base_image_path: imagen que recibirá el estilo <br/>\n",
    "\n",
    "style_reference_mage_path: imagen que proporciona el estilo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(base_image_path))\n",
    "\n",
    "display(Image(style_reference_image_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>\n",
    "Valores mínimos que tendrán los pesos de estilo, base y total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Funciones para pre-procesar y operar las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que abre una imágen, cambia su tamaño y la convierte en un tensor de Tensorflow apropiado.\n",
    "def preprocess_image(image_path):\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "# Función que convierte un tensor en una imagen apropiada.\n",
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Elimina el centrado en cero mediante el promedio del valor del pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Sobre el método: [np.clip()](https://numpy.org/doc/stable/reference/generated/numpy.clip.html)<br/>\n",
    "Sobre el método: [keras.preprocessing.image()](https://keras.io/api/preprocessing/image/)<br/>\n",
    "Sobre el método: [np.expand_dims()](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Arquitectura del modelo para extractor los rasgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Se importan los pesos de la arquitectura VGG-19 pre-entrenada con la base ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Sobre el método: [tf.keras.applications.VGG19](https://keras.io/api/applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "Capas de convolución en la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers\n",
    "#This model contains 16 convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "Se genera un diccionario con el nombre y tipo de cada capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Implementación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Sobre el método: [tf.keras.Model()](https://keras.io/api/models/model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Función de costo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Se definen las siguientes funciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>1.- Gram_matrix\n",
    "        <ul>\n",
    "            <li> Producto entre tensores.\n",
    "        </ul>\n",
    "    <li>2.- Función de Costo para el estilo\n",
    "        <ul>\n",
    "            <li>Mantiene la imagen generada cercana a la textura de la imagen de referencia de estilo.\n",
    "        </ul>\n",
    "    <li>3.- Función de Costo para el contenido\n",
    "        <ul>\n",
    "            <li>Mantiene la representación de alto nivel de la imagen generada cercana a la de la imagen de base.\n",
    "        </ul>\n",
    "    <li>4.- Función de Costo total\n",
    "        <ul>\n",
    "            <li>Función que proporciona coherencia local a la imagen generada.\n",
    "        </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n",
    "\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el método: [tf.reduce_sum()](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/math/reduce_sum)\n",
    "Sobre el método: [tf.pow()](https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/math/pow)\n",
    "Sobre el método: [tf.matmul()](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "Se define la función \"compute_loss()\" para calcular costo en las imágenes: estilo, contenido y combiación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Inicializamos el costo como un arreglo de ceros\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Costo de contenido\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(base_image_features, combination_features)\n",
    "    \n",
    "    # Costo de estilo\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * sl\n",
    "\n",
    "    # Costo total\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "Se utilizará el método de gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el método: [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "    \n",
    "*** Ell parámetro tasa de aprendizaje (learning rate) se reduce en 0.96 cada 100 épocas ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 100.0\n",
    "decay_steps = 100\n",
    "decay_rate = 0.96\n",
    "\n",
    "optimizer = keras.optimizers.SGD(keras.optimizers.schedules.ExponentialDecay(\n",
    "                                initial_learning_rate = initial_learning_rate, \n",
    "                                decay_steps = decay_steps, \n",
    "                                decay_rate = decay_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Implementación del sistema de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "Definición de las capas ocultas con las que se calcularán los costos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista de las capas a utilizar para la determinación del costo referente al estilo.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "\n",
    "# Lista con la capa a utilizar en la determinación del costo referente al contenido de la imagen original.\n",
    "content_layer_name = \"block5_conv2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "Adecuación de las imágenes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image = preprocess_image(base_image_path)\n",
    "\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el método: [tf.Variable()](https://www.tensorflow.org/api_docs/python/tf/Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Transfiriendo el estilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    loss, grads = compute_loss_and_grads(combination_image, base_image, style_reference_image)\n",
    "    \n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        \n",
    "img = deprocess_image(combination_image.numpy())\n",
    "fname= \"final_image.png\"\n",
    "keras.preprocessing.image.save_img(fname,img)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time for training: {:10.4f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Imagen transformada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
