{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad32cdcd",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "Simple examples to ilustrate the concepts of underfitten and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1518d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4ab6c",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "The samples described with the coordinates $(X, Y)$ are generated using the following equation: $$ $$  \n",
    "\n",
    "$$ Y = g(X)+\\eta$$\n",
    "\n",
    "$g(X)$ is the function to generate the samples, $\\eta$ is a noise function defined with normal distribution.\n",
    "\n",
    "To model the correlations between the vairable $X$ and $Y$, we will consider a family of functions $F_\\alpha(W_\\alpha,X)$ that depend on the parameters $W_\\alpha=(w_{\\alpha1}, w_{\\alpha2}, w_{\\alpha3}, ...)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f308424",
   "metadata": {},
   "source": [
    "# Getting samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc52f3",
   "metadata": {},
   "source": [
    "## Modeling the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "N_samples=16\n",
    "\n",
    "#N_samples equally spaced values are generated from 0.05 to 0.95\n",
    "x=np.linspace(0.05,0.95,N_samples)\n",
    "\n",
    "# Gaussian uncorrelated noise\n",
    "# We generate N_samples values at random wint a normal distribution\n",
    "\n",
    "sigma_samples=0.3    #Maximal noise amplitud\n",
    "s = sigma_samples*np.random.randn(N_samples)\n",
    "\n",
    "# polynomial of order 10\n",
    "y= 2*x - 5*x**5 + 15*x**10 + s\n",
    "\n",
    "samples = []\n",
    "for i in range(N_samples) :\n",
    "    samples.append((x[i], y[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732dd0b",
   "metadata": {},
   "source": [
    "### We consider a function to generate the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc528d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(samples, val_ratio=0.2, shuffle=True):\n",
    "    \n",
    "    if shuffle==True:\n",
    "        random.shuffle(samples)\n",
    "             \n",
    "    learn_ratio = int((1.0-val_ratio)*len(samples))\n",
    "    learn = samples[0:learn_ratio]\n",
    "    val = samples[learn_ratio:]\n",
    "\n",
    "    learn_x=[]\n",
    "    learn_y=[]\n",
    "    for i in range(len(learn)):\n",
    "        learn_x.append(learn[i][0])\n",
    "        learn_y.append(learn[i][1])\n",
    "    \n",
    "    x_learn = np.array(learn_x)\n",
    "    y_learn = np.array(learn_y)\n",
    "\n",
    "    val_x=[]\n",
    "    val_y=[]\n",
    "    for i in range(len(val)):\n",
    "        val_x.append(val[i][0])\n",
    "        val_y.append(val[i][1])\n",
    "    \n",
    "    x_val = np.array(val_x)\n",
    "    y_val = np.array(val_y)\n",
    "    \n",
    "    return x_learn, y_learn, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa352969",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "x_learn, y_learn, x_val, y_val = split_samples(samples, val_ratio=val_ratio, shuffle=False)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.ylabel('X', size=20)\n",
    "plt.xlabel('Y', size=20)\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "p1=plt.plot(x_learn, y_learn, \"o\", ms=8, alpha=0.5, label='Training', color='blue')\n",
    "p1=plt.plot(x_val, y_val, \"o\", ms=8, alpha=0.5, label='Training', color='red')\n",
    "plt.legend(['Learn', 'Validation'], loc='upper left', prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cc209",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "x_learn, y_learn, x_val, y_val = split_samples(samples, val_ratio=val_ratio, shuffle=True)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.ylabel('X', size=20)\n",
    "plt.xlabel('Y', size=20)\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "p1=plt.plot(x_learn, y_learn, \"o\", ms=8, alpha=0.5, label='Training', color='blue')\n",
    "p1=plt.plot(x_val, y_val, \"o\", ms=8, alpha=0.5, label='Training', color='red')\n",
    "\n",
    "plt.legend(['Learn', 'Validation'], loc='upper left', prop={'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6082f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_learn.shape)\n",
    "print(y_learn.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d4417dc",
   "metadata": {},
   "source": [
    "n=2\n",
    "\n",
    "poly = PolynomialFeatures(degree= n, include_bias=False, interaction_only=False)\n",
    "x_learn_transf = np.expand_dims(x_learn, axis=1)\n",
    "x_learn_transf = poly.fit_transform(x_learn_transf)\n",
    "x_val_transf = np.expand_dims(x_learn, axis=1)\n",
    "x_val_transf = poly.fit_transform(x_learn_transf)\n",
    "\n",
    "print(x_learn_transf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_samples(x_learn, x_val, degree = 2):\n",
    "    \n",
    "    # The features will defined by the coeficientes of the polinomium\n",
    "    # Therfore, only the X variables will be transformed\n",
    "    x_learn_transf = np.expand_dims(x_learn, axis=1)\n",
    "    x_val_transf = np.expand_dims(x_val, axis=1)\n",
    "    \n",
    "    # Define the number of features for the sample transformation\n",
    "    # It can include or exclude a bias\n",
    "    # It can include interaction between the polynomium term\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False, interaction_only = False)  \n",
    "    \n",
    "    x_learn_transf = poly.fit_transform(x_learn_transf)\n",
    "    x_val_transf = poly.fit_transform(x_val_transf)\n",
    "    \n",
    "    return x_learn_transf, x_val_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(x_learn_pre, x_val_pre, x_learn, y_learn, x_val, y_val, epochs=500, lr=0.01):\n",
    "    \n",
    "    inp = Input((n)) \n",
    "    #since one of the features is 1, we need an extra input\n",
    "    out = Dense(1)(inp)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=Adam(lr=lr), loss=\"mean_squared_error\")\n",
    "    history=model.fit(x_learn, y_learn, epochs=epochs, validation_data=(x_val,y_val), verbose=0)\n",
    "    y_predicted = model.predict(x_learn)\n",
    "    y_predicted = np.squeeze(y_predicted,axis=1)\n",
    "    \n",
    "    x=list(x_learn_pre)\n",
    "    y=list(model.predict(x_learn).squeeze())\n",
    "    pairs=list(zip(x,y))\n",
    "    pairs.sort(key=lambda a: a[0])\n",
    "    pairs\n",
    "    x_plot, y_plot = zip(*pairs)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plt.rc('xtick', labelsize=18) \n",
    "    plt.rc('ytick', labelsize=18)\n",
    "    plt.ylabel('Y', size=20)\n",
    "    plt.xlabel('X', size=20)\n",
    "\n",
    "    ax.scatter(x_learn_pre, y_learn, color='blue')\n",
    "    ax.scatter(x_val_pre, y_val, color='red')\n",
    "    ax.plot(x_plot, y_plot, color=\"green\")\n",
    "    \n",
    "    plt.legend(['Fitting function', 'Learn', 'Validation'], loc='upper left', prop={'size': 16})\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4dd1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n=3\n",
    "x_learn_transf, x_val_transf = transform_samples(x_learn, x_val, degree=n)\n",
    "\n",
    "learning_rate = 0.04\n",
    "epochs =500\n",
    "\n",
    "history=poly_fit(x_learn, x_val, x_learn_transf, y_learn, x_val_transf, y_val, epochs=epochs, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17439694",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Cost function', size=20)\n",
    "plt.ylabel('Cost', size=20)\n",
    "plt.xlabel('Epoch', size=20)\n",
    "plt.legend(['Train', 'Validation'], loc='upper right', prop={'size': 16})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d015ba9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db177cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(xplot, x_learn, y_learn, x_val, y_val, poly_degree):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.ylabel('Y', size=20)\n",
    "    plt.xlabel('X', size=20)\n",
    "    plt.rc('xtick', labelsize=18) \n",
    "    plt.rc('ytick', labelsize=18)\n",
    "\n",
    "\n",
    "    # plot teh samples:\n",
    "    p1=plt.plot(x_learn, y_learn, \"o\", ms=8, alpha=0.5, label='Training', color='blue')\n",
    "    p1=plt.plot(x_val, y_val, 'o', ms=10, alpha=0.5, label='test data', color='red')\n",
    "\n",
    "    # Polynomial Regression\n",
    "    poly = PolynomialFeatures(degree=poly_degree)\n",
    "\n",
    "    # Construct polynomial features\n",
    "    X = poly.fit_transform(x_learn[:,np.newaxis])\n",
    "    clf = linear_model.LinearRegression()\n",
    "    clf.fit(X,y_learn)\n",
    "\n",
    "    Xplot=poly.fit_transform(xplot[:,np.newaxis])\n",
    "    poly_plot=plt.plot(xplot, clf.predict(Xplot), label='Poly', color='green')\n",
    "    \n",
    "    plt.legend(['Learn', 'Validation', 'Fitting function'], loc='upper left', prop={'size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c2a92",
   "metadata": {},
   "source": [
    "## Fit with a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot=np.linspace(0.05,0.95,200)\n",
    "\n",
    "fit_poly(xplot, x_learn, y_learn, x_val, y_val, poly_degree=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6498d1",
   "metadata": {},
   "source": [
    "##  Polynomial Regression: seconth order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3abf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(xplot, x_learn, y_learn, x_val, y_val, poly_degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2571c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677723a8",
   "metadata": {},
   "source": [
    "##  Polynomial Regression: third order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33603c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(xplot, x_learn, y_learn, x_val, y_val, poly_degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efdb0e",
   "metadata": {},
   "source": [
    "##  Polynomial Regression: fifth order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3906b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(xplot, x_learn, y_learn, x_val, y_val, poly_degree=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13073248",
   "metadata": {},
   "source": [
    "##  Polynomial Regression: tenth order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(xplot, x_learn, y_learn, x_val, y_val, poly_degree=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d5c4f",
   "metadata": {},
   "source": [
    "##  Polynomial Regression: eleventh order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c956a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(xplot, x_learn, y_learn, x_val, y_val, poly_degree=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680b8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
